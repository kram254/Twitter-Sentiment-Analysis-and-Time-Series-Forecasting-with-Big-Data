{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c90a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e4ca1",
   "metadata": {},
   "source": [
    "### Initializing Spark session to connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SentimentTimeSeries\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/TwitterAnalysisDB.RawData\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/TwitterAnalysisDB.RawData\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acef82f",
   "metadata": {},
   "source": [
    "### Reading data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75b396",
   "metadata": {},
   "source": [
    "### Sentiment analysis preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f75dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0aa8af",
   "metadata": {},
   "source": [
    "### Assemble pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdfb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc9228",
   "metadata": {},
   "source": [
    "### Train the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab6657",
   "metadata": {},
   "source": [
    "### Assume the sentiment data is already in 'df' with 'timestamp' and 'sentiment' columns\n",
    "#### Select relevant columns and convert to pandas for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44aaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.select(\"timestamp\", \"sentiment\").withColumnRenamed(\"timestamp\", \"ds\").withColumnRenamed(\"sentiment\", \"y\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9aa81",
   "metadata": {},
   "source": [
    "### # Make sure the pandas dataframe is sorted and has the correct datetime type for the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df['ds'] = pd.to_datetime(pandas_df['ds'])\n",
    "pandas_df.set_index('ds', inplace=True)\n",
    "pandas_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77456b7b",
   "metadata": {},
   "source": [
    "### SARIMAX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_model = SARIMAX(pandas_df['y'], order=(1, 0, 0), seasonal_order=(1, 1, 0, 7))\n",
    "sarimax_result = sarimax_model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58403c",
   "metadata": {},
   "source": [
    "#### Perform forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_week = sarimax_result.get_forecast(steps=7)\n",
    "forecast_month = sarimax_result.get_forecast(steps=30)\n",
    "forecast_3month = sarimax_result.get_forecast(steps=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde91fc5",
   "metadata": {},
   "source": [
    "### Convert forecasts to Spark DataFrames and save to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_week_df = spark.createDataFrame(forecast_week.summary_frame())\n",
    "forecast_week_df.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"forecast_week_collection\").save()\n",
    "forecast_month_df = spark.createDataFrame(forecast_month.summary_frame())\n",
    "forecast_month_df.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"forecast_month_collection\").save()\n",
    "forecast_3month_df = spark.createDataFrame(forecast_3month.summary_frame())\n",
    "forecast_3month_df.write.format(\"mongo\").mode(\"overwrite\").option(\"collection\", \"forecast_3month_collection\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400f89a",
   "metadata": {},
   "source": [
    "### Close the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
